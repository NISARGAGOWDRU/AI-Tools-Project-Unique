# ğŸ™ï¸ Voice Emotion Recognition

A machine learning project that recognizes emotions from voice/audio signals using advanced audio feature extraction and classification algorithms.

![Python](https://img.shields.io/badge/Python-3.8+-blue?style=flat-square)
![ML](https://img.shields.io/badge/ML-TensorFlow-orange?style=flat-square)
![License](https://img.shields.io/badge/License-MIT-green?style=flat-square)

---

## âœ¨ Features

- ğŸµ **Audio Feature Extraction** - Extracts MFCC, spectral features, and more
- ğŸ¤– **Deep Learning Model** - Neural network for emotion classification
- ğŸ“Š **Real-time Analysis** - Process audio files and predict emotions
- ğŸ“ˆ **Multiple Emotions** - Detect Happy, Sad, Angry, Neutral, Fearful, etc.
- ğŸ“ **Batch Processing** - Process multiple audio files at once

---

## ğŸ› ï¸ Technologies Used

- **Python 3.8+**
- **TensorFlow/Keras** - Deep learning framework
- **Librosa** - Audio analysis library
- **NumPy, Pandas** - Data processing
- **Scikit-learn** - Machine learning utilities
- **Matplotlib** - Data visualization

---

## ğŸš€ Quick Start

### Installation

```bash
# Clone repository
git clone https://github.com/NISARGAGOWDRU/Voice-Emotion-Recognition.git
cd Voice-Emotion-Recognition

# Create virtual environment
python -m venv .venv
.\.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt
```

### Running the Project

```bash
# Run emotion recognition on audio file
python main.py --audio path/to/audio.wav

# Train model on dataset
python train.py --dataset path/to/dataset

# Evaluate model performance
python evaluate.py --model path/to/model.h5
```

---

## ğŸ“Š Usage Example

```python
from emotion_recognizer import EmotionRecognizer

# Initialize recognizer
recognizer = EmotionRecognizer(model_path='model.h5')

# Predict emotion from audio
emotion, confidence = recognizer.predict('audio.wav')
print(f"Emotion: {emotion}, Confidence: {confidence:.2%}")
```

---

## ğŸ“ˆ Model Performance

| Emotion | Precision | Recall | F1-Score |
|---------|-----------|--------|----------|
| Happy   | 0.92      | 0.89   | 0.90     |
| Sad     | 0.88      | 0.91   | 0.89     |
| Angry   | 0.94      | 0.92   | 0.93     |
| Neutral | 0.85      | 0.87   | 0.86     |

---

## ğŸ“ Project Structure

```
Voice-Emotion-Recognition/
â”œâ”€â”€ main.py                    # Main prediction script
â”œâ”€â”€ train.py                   # Model training script
â”œâ”€â”€ evaluate.py                # Model evaluation
â”œâ”€â”€ models/
â”‚   â””â”€â”€ emotion_model.h5       # Trained model
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train/                 # Training data
â”‚   â””â”€â”€ test/                  # Test data
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ audio_processor.py     # Audio processing
â”‚   â””â”€â”€ feature_extractor.py   # Feature extraction
â””â”€â”€ requirements.txt           # Dependencies
```

---

## ğŸ¯ Supported Emotions

- ğŸ˜Š **Happy**
- ğŸ˜¢ **Sad**
- ğŸ˜  **Angry**
- ğŸ˜ **Neutral**
- ğŸ˜¨ **Fearful**
- ğŸ¤¢ **Disgusted**
- ğŸ˜² **Surprised**

---

## ğŸ“– Documentation

For detailed documentation, see [DOCS.md](DOCS.md)

---

## ğŸ¤ Contributing

Contributions welcome! Please fork and submit pull requests.

---

## ğŸ“„ License

MIT License - see [LICENSE](LICENSE) for details

---

**Made with â¤ï¸ by NISARGA GOWDRU**
